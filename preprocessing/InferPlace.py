import numpy as np
import matplotlib.pyplot as plt
import ReadData


data = ReadData.ReadData()

baseDir = "D:/Research/Result/checkin/"
wordTopicCountPath = baseDir+"word_topic_count_1542169953208.txt"
topicDocsPath = baseDir+"topic_doc_dis_1542169953208.csv"
topicSequencesPath = baseDir+"topic_seq_1542169953208.csv"

wordTopicDistribution = data.read_word_topic(wordTopicCountPath)

words = data.words
topicDocsDis = data.read_topic_doc(topicDocsPath)
topicSequences = data.read_topic_sequence(topicSequencesPath)

trainPath = "D:\Research\Dataset\checkin/user_checkin_above_10x10x5_us_train - Copy.txt"
train = data.read_train(trainPath)

testPath = "D:\Research\Dataset\checkin/user_checkin_above_10x10x5_us_test - Copy.txt"
test = data.read_test(testPath)

wordByTopicProbabilitiesAll = []
wordByDocProbabilitiesAvgAll = []
docIndex = 0
for doc in train:
    train_word_set = list(set(doc))
    wordByDocProbabilities = []
    wordByTopicProbabilities = []
    for word in train_word_set:
        wordIndex = words.index(word)
        wordByDocProbabilities.append(float(doc.count(word)) / len(doc))
        wordDistribution = wordTopicDistribution[words.index(word)]
        wordByTopicSum = 0.0
        topicIndex = 0
        for topicDisOfDoc in topicDocsDis[docIndex]:
            if topicDisOfDoc > 0 and wordDistribution[topicIndex] > 0:
                wordByTopicSum += topicDisOfDoc * wordDistribution[topicIndex]
            topicIndex += 1
        wordByTopicProbabilities.append(wordByTopicSum)
    # print(wordByDocProbabilities)
    # print(wordByTopicProbabilities)
    wordByDocProbabilitiesAvg = np.average(wordByDocProbabilities)
    wordByDocProbabilitiesAvgAll.append(wordByDocProbabilitiesAvg)

    print(docIndex + 1, end="\t")
    print(len(doc), end="\t")
    print(len(train_word_set), end="\t")
    # average probability of word in a doc
    # higher value means that few words, high probability
    # lower value means that many words, low probability
    # print(round(wordByDocProbabilitiesAvg, 6), end="\t")
    wordRatioDocTopic = []
    wordByTopicRelativeProbabilities = []
    wordByTopicProbabilitiesSum = sum(wordByTopicProbabilities)

    # average probability of word generated by topic * word matrix
    # higher value means it will affect the prediction
    # print(round(np.average(wordByTopicProbabilities), 6), end="\t")
    print(format(np.average(wordByTopicProbabilities), '.6f'), end="\t")
    wordByTopicProbabilitiesAll.append(wordByTopicProbabilitiesSum)

    print(len(set(train_word_set) & set(test[docIndex])))

    wordIndex = 0
    for wordByTopic in wordByTopicProbabilities:
        wordByTopicRelativeProbabilities.append(wordByTopic / wordByTopicProbabilitiesSum)
        wordRatioDocTopic.append(wordByDocProbabilities[wordIndex] / (wordByDocProbabilities[wordIndex] +
                                                                      wordByTopic / wordByTopicProbabilitiesSum))
        wordIndex += 1
    # print(wordByTopicRelativeProbabilities)
    # print(wordRatioDocTopic)
    # print(train_word_set)
    docIndex += 1
    if docIndex > 100:
        break
print(np.average(wordByTopicProbabilitiesAll))
print(np.max(wordByTopicProbabilitiesAll))


# n, bins, patches = plt.hist(wordByTopicProbabilitiesAll, 2000, density=False, facecolor='g', alpha=0.75)
# plt.grid(True)
# plt.show()
